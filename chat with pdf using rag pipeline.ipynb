{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jawadshaik73/Rag-pipeline/blob/main/chat%20with%20pdf%20using%20rag%20pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests PyMuPDF sentence-transformers faiss-cpu transformers torch\n"
      ],
      "metadata": {
        "id": "vrgAzYdegN_e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14f481c3-9695-49ba-e67c-8a2d1abd1246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.25.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0.post1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.27.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "ROmkYA1kB7Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc.load_page(page_num)\n",
        "        text += page.get_text()\n",
        "    return text"
      ],
      "metadata": {
        "id": "S4sx2rUrB-R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, chunk_size=300):\n",
        "    words = text.split()\n",
        "    return [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]"
      ],
      "metadata": {
        "id": "Bvq8wqtYCDCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_faiss_index(chunks, model_name=\"all-MiniLM-L6-v2\"):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    embeddings = model.encode(chunks, convert_to_tensor=True).cpu().numpy()\n",
        "\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "    return index, model"
      ],
      "metadata": {
        "id": "C9C1woh8CHx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_similar_chunks(query, index, model, chunks, top_k=3):\n",
        "    query_embedding = model.encode([query], convert_to_tensor=True).cpu().numpy()\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "    return [chunks[i] for i in indices[0]]"
      ],
      "metadata": {
        "id": "x8D18zaBCIXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(query, retrieved_chunks, model, tokenizer, max_new_tokens=100, context_limit=1024):\n",
        "    context = \"\\n\".join(retrieved_chunks)\n",
        "    if len(context) > context_limit:\n",
        "        context = context[:context_limit]\n",
        "\n",
        "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    inputs = inputs.to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "6n-WuztzCRil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_pipeline():\n",
        "    pdf_path = \"/DATA MINING UNIT-1 NOTES.pdf\"\n",
        "    content = extract_text_from_pdf(pdf_path)\n",
        "    if not content:\n",
        "        print(\"No content retrieved from the PDF. Exiting.\")\n",
        "        return\n",
        "    chunks = chunk_text(content)\n",
        "    print(f\"Extracted {len(chunks)} chunks from the PDF content.\")\n",
        "    index, embedding_model = create_faiss_index(chunks)\n",
        "    print(\"Loading GPT-2 model...\")\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    llm_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "    llm_model = llm_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"System is ready. Type your questions below!\")\n",
        "    while True:\n",
        "        user_query = input(\"\\nEnter your question (or 'exit' to quit): \").strip()\n",
        "        if user_query.lower() == \"exit\":\n",
        "            print(\"Exiting the chat. Goodbye!\")\n",
        "            break\n",
        "        if not user_query:\n",
        "            print(\"Please enter a valid question.\")\n",
        "            continue\n",
        "        retrieved_chunks = search_similar_chunks(user_query, index, embedding_model, chunks)\n",
        "        if not retrieved_chunks:\n",
        "            print(\"No relevant information found.\")\n",
        "            continue\n",
        "        response = generate_response(user_query, retrieved_chunks, llm_model, tokenizer)\n",
        "        print(\"\\nResponse:\")\n",
        "        print(response)\n",
        "if __name__ == \"__main__\":\n",
        "    main_pipeline()"
      ],
      "metadata": {
        "id": "-2fFqEJIgN5E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "541033a8-d77d-40e4-b406-eb535bff0b1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted 15 chunks from the PDF content.\n",
            "Loading GPT-2 model...\n",
            "System is ready. Type your questions below!\n",
            "\n",
            "Response:\n",
            "Context: Faculty: Mr. D. Krishna, Associate Professor CSE Dept DATA MINING UNIT-I Introduction to Data Mining: Data mining is the process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. The information or knowledge extracted so can be used for any of the following applications:  Market Analysis  Fraud Detection  Customer Retention  Production Control  Science Exploration Data Mining Applications: Data mining is highly useful in the following domains:  Market Analysis and Management  Corporate Analysis & Risk Management  Fraud Detection Apart from these, data mining can also be used in the areas of production control, customer retention, science exploration, sports, astrology, and Internet Web Surf- Aid. Knowledge discovery in databases (KDD): Knowledge discovery in databases (KDD) is the process of discovering useful knowledge from a collection of data. Data Cleaning: The noise and inconsistent data is removed. Data Integra\n",
            "\n",
            "Question: what is KDD\n",
            "Answer: Kdd is a data analysis tool that can help you to identify patterns and patterns of information in a large set of datasets. Kd is used to analyze data from various sources, such as:\n",
            "- databases, web sites, social media, etc.\n",
            "The data can then be analyzed by Kdb. This is done by using KDB to perform a series of analyses. In this way, Kdf can identify the patterns that are most important to you. For example, if you have a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TaOwml4F1Pcd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}